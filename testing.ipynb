{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing IAS_mapper::samparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        return len(f.readlines())\n",
    "    \n",
    "def cigar_caller(mystring):\n",
    "    mynumb = \"\"\n",
    "    total = 0\n",
    "    for char in mystring:\n",
    "        if char.isdigit() == True:\n",
    "            mynumb += char\n",
    "        elif char == \"M\":\n",
    "            total += int(mynumb)\n",
    "            mynumb = \"\"\n",
    "        elif char == \"I\":\n",
    "            total -= int(mynumb)\n",
    "            mynumb = \"\"\n",
    "        elif char == \"D\":\n",
    "            total += int(mynumb)*2\n",
    "            mynumb = \"\"\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = Path(\"toy-data\")\n",
    "sam_files = [file for file in toy_data.iterdir() if file.suffix == '.sam']\n",
    "print(sam_files)\n",
    "irl_rt = Path('toy-data/B16-1_1-RT_IRL.sam')\n",
    "irr_rt = Path('toy-data/B16-1_1-RT_IRR.sam')\n",
    "irl_s = Path('toy-data/B16-1_1-S_IRL.sam')\n",
    "irr_s = Path('toy-data/B16-1_1-S_IRL.sam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decipher these flags\n",
    "# https://en.wikipedia.org/wiki/SAM_(file_format)#cite_note-spec-4\n",
    "read1_flags = [69,73,89,99,81,83,89]\n",
    "[ print(i, bin(i)) for i in read1_flags ]\n",
    "read2_flags = [161,163,169,145,147]\n",
    "[ print(i, bin(i)) for i in read2_flags ]\n",
    "read1_flags_pos = [69,73,89,99]\n",
    "read1_flags_neg = [81,83,89]\n",
    "\n",
    "read2_flags_pos = [161,163,169]\n",
    "read2_flags_neg = [145,147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # results[chrom:address(-/+)] = [# in IRL, # in IRR]\n",
    "ligation = {}\n",
    "IRLmax = 0\n",
    "IRRmax = 0\n",
    "\n",
    "file = irl_rt\n",
    "linecount = file_len(file)\n",
    "with open(file, \"r\") as SAM:\n",
    "    count = 0\n",
    "    count_reads = 0\n",
    "    len12 = 0\n",
    "    has_flags = 0\n",
    "    for line1, line2 in itertools.zip_longest(SAM, SAM, fillvalue=''):\n",
    "        if linecount - count > 2:\n",
    "            count += 2\n",
    "            count_reads += 1\n",
    "            read1 = line1.split(\"\\t\")\n",
    "            read2 = line2.split(\"\\t\")\n",
    "            flag1 = int(read1[1])\n",
    "            flag2 = int(read2[1])\n",
    "            if flag1 in read1_flags:\n",
    "                flag = flag1\n",
    "                chrom  = read1[2]\n",
    "                address = int(read1[3])\n",
    "                length = cigar_caller(read1[5])\n",
    "                seq = read1[9]\n",
    "            elif flag2 in read1_flags:\n",
    "                flag = flag2\n",
    "                chrom  = read2[2]\n",
    "                address = int(read2[3])\n",
    "                length = cigar_caller(read2[5])\n",
    "                seq = read2[9]\n",
    "            else:\n",
    "                # length = 0\n",
    "                continue\n",
    "            \n",
    "            #Runs if first line contains the mapping data for the transposon read\n",
    "            #Executes if transposon IRL read mapped to + strand\n",
    "            has_flags += 1\n",
    "            \n",
    "            # there is a minimum len input for hisat2, so it has to be larger than 12\n",
    "            if length >= 12:\n",
    "                len12 += 1\n",
    "                # if the read flag is in the positve strand\n",
    "                if flag in read1_flags_pos:\n",
    "                    # and the sequence starts with TA\n",
    "                    if seq[0:2] == \"TA\":\n",
    "                        # flips strand for IRL\n",
    "                        tag = f\"{chrom}:{address}-\"\n",
    "                        # add tag to results\n",
    "                        if tag in results:\n",
    "                            results[tag][0] += 1\n",
    "                            if results[tag][0] > IRLmax:\n",
    "                                IRLmax = results[tag][0]\n",
    "                            mapcheck = True\n",
    "                        else:\n",
    "                            results[tag] = [1, 0]\n",
    "                            mapcheck = True\n",
    "                        # add length to ligation\n",
    "                        if tag in ligation:\n",
    "                            if length not in ligation[tag][0]:\n",
    "                                ligation[tag][0].append(length)\n",
    "                        else:\n",
    "                            ligation[tag] = [[], []]\n",
    "                            ligation[tag][0].append(length)\n",
    "                            \n",
    "                # else if the read flag is in the negative strand\n",
    "                elif flag in read1_flags_neg:\n",
    "                    if seq[-2:] == \"TA\":\n",
    "                        # flips strand for IRL\n",
    "                        tag = f\"{chrom}:{address + (length - 2)}+\"\n",
    "                        if tag in results:\n",
    "                            results[tag][0] += 1\n",
    "                            if results[tag][0] > IRLmax:\n",
    "                                IRLmax = results[tag][0]\n",
    "                            mapcheck = True\n",
    "                        else:\n",
    "                            results[tag] = [1,0]\n",
    "                            mapcheck = True\n",
    "                        if tag in ligation:\n",
    "                            if length not in ligation[tag][0]:\n",
    "                                ligation[tag][0].append(length)\n",
    "                        else:\n",
    "                            ligation[tag] = [[],[]]\n",
    "                            ligation[tag][0].append(length)\n",
    "                \n",
    "                # paired read only if the transposon read cannot be mapped precisely\n",
    "                if mapcheck == False: \n",
    "                    if flag1 in read2_flags:\n",
    "                        flag = flag1\n",
    "                        chrom = read1[2]\n",
    "                        address = read1[3]\n",
    "                        length = cigar_caller(read1[5])\n",
    "                        seq = read1[9]\n",
    "                    elif flag2 in read2_flags:\n",
    "                        flag = flag2\n",
    "                        chrom = read2[2]\n",
    "                        address = read2[3]\n",
    "                        length = cigar_caller(read2[5])\n",
    "                        seq = read2[9]\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    if length >= 10:\n",
    "                        if flag in read2_flags_pos:\n",
    "                            if seq[0:2] == \"TA\":\n",
    "                                tag = f\"{chrom}:{address}-\"\n",
    "                                if tag in results:\n",
    "                                    results[tag][0] += 1\n",
    "                                    if results[tag][0] > IRLmax:\n",
    "                                        IRLmax = results[tag][0]\n",
    "                                    mapcheck = True\n",
    "                                else:\n",
    "                                    results[tag] = [1,0]\n",
    "                                    mapcheck = True\n",
    "                                if tag in ligation:\n",
    "                                    if length not in ligation[tag][0]:\n",
    "                                        ligation[tag][0].append(length)\n",
    "                                else:\n",
    "                                    ligation[tag] = [[],[]]\n",
    "                                    ligation[tag][0].append(length)\n",
    "                                    \n",
    "                        elif flag in read2_flags_neg:\n",
    "                            if seq[-2:] == \"TA\":\n",
    "                                tag = f\"{chrom}:{address + (length - 2)}+\"\n",
    "                                if tag in results:\n",
    "                                    results[tag][0] += 1\n",
    "                                    if results[tag][0] > IRLmax:\n",
    "                                        IRLmax = results[tag][0]\n",
    "                                    mapcheck = True\n",
    "                                else:\n",
    "                                    results[tag] = [1,0]\n",
    "                                    mapcheck = True\n",
    "                                if tag in ligation:\n",
    "                                    if length not in ligation[tag][0]:\n",
    "                                        ligation[tag][0].append(length)\n",
    "                                else:\n",
    "                                    ligation[tag] = [[],[]]\n",
    "                                    ligation[tag][0].append(length)\n",
    "print(count_reads)\n",
    "print(has_flags)\n",
    "print(len12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "samfile = pysam.AlignmentFile('toy-data/2020_SB_bam/B16-1_1-RT_IRL.bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in samfile.head(n=2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    \n",
    "    if read.is_paired:\n",
    "        print(read)\n",
    "        \n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile.get_index_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple of the lengths of the reference sequences.\n",
    "# The lengths are in the same order as pysam.AlignmentFile.references\n",
    "samfile.lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile.references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int with total number of unmapped reads according to the statistics recorded in the index.\n",
    "# This number of reads includes the number of reads without coordinates. \n",
    "samfile.unmapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int with total number of reads without coordinates according to the statistics recorded in the index\n",
    "# i.e., the statistic printed for “*” by the samtools idxstats command\n",
    "samfile.nocoordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    \n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.cigarstring)\n",
    "        print(read.cigartuples)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "\n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.get_aligned_pairs())\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "\n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.get_cigar_stats())\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "\n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.get_forward_qualities())\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads mapped to the reverse strand are stored reverse complemented in the BAM file.\n",
    "# This method returns such reads reverse complemented back to their original orientation.\n",
    "\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "\n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.seq)\n",
    "        print(read.get_forward_sequence())\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer read length from CIGAR alignment.\n",
    "# This method deduces the read length from the CIGAR alignment including hard-clipped bases.\n",
    "\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "\n",
    "    if read.is_paired:\n",
    "        # each read is pysam.AlinedSegment object\n",
    "        print(read.infer_read_length())\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    print(read.is_read1)\n",
    "\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    print(read.is_read2)\n",
    "\n",
    "    if i == 1:\n",
    "        break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    print(read.is_forward)\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    print(read.next_reference_start)\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ta = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    if read.is_mapped and read.is_read1:\n",
    "        tmp = read.get_forward_sequence()\n",
    "        if tmp[:2] =='TA':\n",
    "            count_ta += 1\n",
    "print(count_ta)\n",
    "print(samfile.mapped)\n",
    "print(samfile.unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    if read.is_mapped and read.is_read1 and not read.is_forward:\n",
    "        if read.seq[-2:] == 'TA':\n",
    "            print(read.is_forward)\n",
    "            print(read.seq)\n",
    "            print(read.get_forward_sequence())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ta = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    # each read is pysam.AlinedSegment object\n",
    "    if read.is_mapped and read.is_read2:\n",
    "        tmp = read.get_forward_sequence()\n",
    "        if tmp[:2] =='TA':\n",
    "            count_ta += 1\n",
    "print(i)\n",
    "print(count_ta)\n",
    "print(samfile.mapped)\n",
    "print(samfile.unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ta = 0\n",
    "count_dis = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    if read.is_mapped:\n",
    "        tmp = read.get_forward_sequence()\n",
    "        if tmp[:2] == 'TA':\n",
    "            count_ta += 1\n",
    "    else:\n",
    "        count_dis += 1\n",
    "        print(read.mate_is_mapped, read.is_mapped)\n",
    "        print(read.flag)\n",
    "print()\n",
    "print(i)\n",
    "print(count_dis)\n",
    "print(count_ta)\n",
    "print(samfile.mapped)\n",
    "print(samfile.unmapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = [69, 101, 133, 165]\n",
    "[ print(bin(flag)) for flag in flags ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a pd.DataFrame with stats per file using pysam commands to fill out the columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pysam.readthedocs.io/en/latest/api.html#api\n",
    "\n",
    "https://en.wikipedia.org/wiki/SAM_(file_format)\n",
    "\n",
    "Determine strand orientation using read number (read1 vs. read2) and is_forward (or is + strand)\n",
    "\n",
    "How to link read1 to read2? samfile.mate(read)\n",
    "\n",
    "Does cigartuples give the same as what we are expecting in cigar_caller?\n",
    "\n",
    "Find discrepencies in samfile.mapped and samfile.fetch(). I think what is happening is one of the reads is mapped and the other paired read (mate) is unmapped\n",
    "\n",
    "Find discrepencies in samfile.unmapped and samfile.nocoordinates. Could be the same as above\n",
    "\n",
    "Get per contig (chromosome) stats with samfile.get_index_statistics()\n",
    "\n",
    "Sense strand is forward, or the (+) strand\n",
    "\n",
    "What we want to have in the dataframe...?\n",
    "- mapped reads1\n",
    "- mapped reads2\n",
    "- mapped reads1 starts TA\n",
    "- mapped reads2 starst TA\n",
    "- unmapped reads1\n",
    "- unmapped reads2\n",
    "\n",
    "For each insertion\n",
    "\n",
    "- save the transposon orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd\n",
    "samfile = pysam.AlignmentFile('toy-data/2020_SB-bam/B16-1_1-RT_IRL.bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ read for i, read in enumerate(samfile.fetch()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    if read.is_mapped:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    if read.is_mapped and read.mate_is_mapped:\n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    if read.is_mapped and read.mate_is_mapped:\n",
    "        print(read.get_forward_sequence())\n",
    "        print(samfile.mate(read).get_forward_sequence())\n",
    "        print(read.cigartuples)\n",
    "        print(samfile.mate(read).cigartuples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, read in enumerate(samfile.fetch()):\n",
    "    if not read.is_mapped and read.mate_is_mapped:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRCm39 \n",
    "# https://www.ncbi.nlm.nih.gov/assembly/GCF_000001635.27/\n",
    "# https://genome.ucsc.edu/cgi-bin/hgTracks?chromInfoPage=&hgsid=1560703641_1YwiSDzyFEZ8nuDrTobTnwtYvReT\n",
    "\n",
    "refseq2chr = {\n",
    "    'NC_000067.7': 'chr1',\n",
    "    'NC_000068.8': 'chr2',\n",
    "    'NC_000069.7': 'chr3',\n",
    "    'NC_000070.7': 'chr4',\n",
    "    'NC_000071.7': 'chr5',\n",
    "    'NC_000072.7': 'chr6',\n",
    "    'NC_000073.7': 'chr7',\n",
    "    'NC_000074.7': 'chr8',\n",
    "    'NC_000075.7': 'chr9',\n",
    "    'NC_000076.7': 'chr10',\n",
    "    'NC_000077.7': 'chr11',\n",
    "    'NC_000078.7': 'chr12',\n",
    "    'NC_000079.7': 'chr13',\n",
    "    'NC_000080.7': 'chr14',\n",
    "    'NC_000081.7': 'chr15',\n",
    "    'NC_000082.7': 'chr16',\n",
    "    'NC_000083.7': 'chr17',\n",
    "    'NC_000084.7': 'chr18',\n",
    "    'NC_000085.7': 'chr19',\n",
    "    'NC_000086.8': 'chrX',\n",
    "    'NC_000087.8': 'chrY',\n",
    "    'NC_005089.1': 'chrM'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the insertions stats (direction, +/-, and all that)\n",
    "def get_insertion_properties(insertion):\n",
    "    res = {\n",
    "        'name': [insertion.query_name],\n",
    "        'chr':[refseq2chr[insertion.reference_name]],\n",
    "        'pos': [insertion.reference_start],  # 0-based left most coordinate\n",
    "        'strand +': [insertion.is_forward],  # For IRR: + if forward, - if not. For IRL this is reversed\n",
    "        'ref length': [insertion.reference_length],\n",
    "        'query length': [insertion.infer_query_length()],  # does not include hard-clipped bases\n",
    "        'read length': [insertion.infer_read_length()],  # does include hard-clipped bases. should be equal to len(query_sequence)\n",
    "        'quality': [insertion.mapping_quality],\n",
    "    }\n",
    "    res = pd.DataFrame.from_dict(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "insertions = []\n",
    "for i, read1 in enumerate(samfile.fetch()):\n",
    "    \n",
    "    # don't even bother with reads that aren't paired (though this shouldn't ever happen, doens't hurt to be safe)\n",
    "    if not read1.is_paired:\n",
    "        continue\n",
    "    \n",
    "    # don't use reads that are mapped to different chroms\n",
    "    if read1.reference_name != read2.reference_name:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # only look at read 1\n",
    "    if read1.is_read1:\n",
    "        # if read 1 is mapped, continue with this read\n",
    "        if read1.is_mapped:\n",
    "            read = read1\n",
    "        # else read 1 isn't mapped, check if the mate (read 2) is mapped and use that read\n",
    "        elif read1.mate_is_mapped:\n",
    "            read = samfile.mate(read1)\n",
    "        # if neither are mapped, then we won't process these paired reads\n",
    "        else:\n",
    "            continue\n",
    "    # skip read 2 so that we aren't doubling our insertions\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # check if the contig (chromosome) is what we want\n",
    "    if read.reference_name not in refseq2chr.keys():\n",
    "        continue\n",
    "    \n",
    "    # check if read begins with TA\n",
    "    if read.get_forward_sequence()[:2] == 'TA':\n",
    "        insert_properties = get_insertion_properties(read)\n",
    "    # if not, check if the mate read is mapped and then check if that read starts with a TA\n",
    "    elif read.mate_is_mapped:\n",
    "        mate_read = samfile.mate(read)\n",
    "        if mate_read.get_forward_sequence()[:2] == 'TA':\n",
    "            insert_properties = get_insertion_properties(mate_read)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # record this as an insertion at a site\n",
    "    insertions.append(insert_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual insertions into counts of insertions per site\n",
    "insertions_df = pd.concat(insertions, axis=0).reset_index(drop=True)\n",
    "display(insertions_df.set_index(['chr', 'pos', 'strand +']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = insertions_df.groupby(by=['chr', 'pos'], as_index=False, dropna=False)['name'].count()\n",
    "tmp1 =tmp[['chr', 'pos']]\n",
    "tmp1['count'] = tmp[['name']]\n",
    "display(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = insertions_df.groupby(by=['chr', 'pos'], as_index=False, dropna=False).mean(numeric_only=True)\n",
    "int_cols = ['strand +', 'ref length', 'query length', 'read length', 'quality']\n",
    "tmp[int_cols] = tmp[int_cols].astype(int)\n",
    "res_df = tmp1.merge(tmp, on=['chr', 'pos'])\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "\n",
    "# GRCm39 \n",
    "# https://www.ncbi.nlm.nih.gov/assembly/GCF_000001635.27/\n",
    "# https://genome.ucsc.edu/cgi-bin/hgTracks?chromInfoPage=&hgsid=1560703641_1YwiSDzyFEZ8nuDrTobTnwtYvReT\n",
    "\n",
    "refseq2chr = {\n",
    "    'NC_000067.7': 'chr1',\n",
    "    'NC_000068.8': 'chr2',\n",
    "    'NC_000069.7': 'chr3',\n",
    "    'NC_000070.7': 'chr4',\n",
    "    'NC_000071.7': 'chr5',\n",
    "    'NC_000072.7': 'chr6',\n",
    "    'NC_000073.7': 'chr7',\n",
    "    'NC_000074.7': 'chr8',\n",
    "    'NC_000075.7': 'chr9',\n",
    "    'NC_000076.7': 'chr10',\n",
    "    'NC_000077.7': 'chr11',\n",
    "    'NC_000078.7': 'chr12',\n",
    "    'NC_000079.7': 'chr13',\n",
    "    'NC_000080.7': 'chr14',\n",
    "    'NC_000081.7': 'chr15',\n",
    "    'NC_000082.7': 'chr16',\n",
    "    'NC_000083.7': 'chr17',\n",
    "    'NC_000084.7': 'chr18',\n",
    "    'NC_000085.7': 'chr19',\n",
    "    'NC_000086.8': 'chrX',\n",
    "    'NC_000087.8': 'chrY',\n",
    "    'NC_005089.1': 'chrM'\n",
    "}\n",
    "\n",
    "# record the insertions stats (direction, +/-, and all that)\n",
    "def get_insertion_properties(insertion, chrdict):\n",
    "    res = {\n",
    "        'name': [insertion.query_name],\n",
    "        'chr':[chrdict[insertion.reference_name]],\n",
    "        'pos': [insertion.reference_start],  # 0-based left most coordinate\n",
    "        'strand +': [insertion.is_forward],  # TODO: For IRR: + if forward, - if not. For IRL this is reversed\n",
    "        'ref length': [insertion.reference_length],\n",
    "        'query length': [insertion.infer_query_length()],  # does not include hard-clipped bases\n",
    "        'read length': [insertion.infer_read_length()],  # does include hard-clipped bases. should be equal to len(query_sequence)\n",
    "        'quality': [insertion.mapping_quality],\n",
    "    }\n",
    "    res = pd.DataFrame.from_dict(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_bam(bam_file, chr_dict, is_irr):\n",
    "    bam = pysam.AlignmentFile(bam_file)\n",
    "    insertions = []\n",
    "    for i, read1 in enumerate(samfile.fetch()):\n",
    "        \n",
    "        # don't even bother with reads that aren't paired (though this shouldn't ever happen, doens't hurt to be safe)\n",
    "        if not read1.is_paired:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        # only look at read 1\n",
    "        if read1.is_read1:\n",
    "            # if read 1 is mapped, continue with this read\n",
    "            if read1.is_mapped:\n",
    "                read = read1\n",
    "            # else read 1 isn't mapped, check if the mate (read 2) is mapped and use that read\n",
    "            elif read1.mate_is_mapped:\n",
    "                read = samfile.mate(read1)\n",
    "            # if neither are mapped, then we won't process these paired reads\n",
    "            else:\n",
    "                continue\n",
    "        # skip read 2 so that we aren't doubling our insertions\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # check if the contig (chromosome) is what we want\n",
    "        if read.reference_name not in chr_dict.keys():\n",
    "            continue\n",
    "        \n",
    "        # check if read begins with TA\n",
    "        if read.get_forward_sequence()[:2] == 'TA':\n",
    "            insert_properties = get_insertion_properties(read, chr_dict)\n",
    "        # if not, check if the mate read is mapped and then check if that read starts with a TA\n",
    "        elif read.mate_is_mapped:\n",
    "            mate_read = samfile.mate(read)\n",
    "            if mate_read.get_forward_sequence()[:2] == 'TA':\n",
    "                insert_properties = get_insertion_properties(mate_read, chr_dict)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # record this as an insertion at a site\n",
    "        insertions.append(insert_properties)\n",
    "        \n",
    "    insertions_df = pd.concat(insertions, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # group together reads that occur at the same chr, pos, and strand. Get the counts\n",
    "    tmp = insertions_df.groupby(by=['chr', 'pos', 'strand +'], as_index=False, dropna=False)['name'].count()\n",
    "    tmp1 = tmp[['chr', 'pos', 'strand +']]\n",
    "    tmp1['count'] = tmp[['name']]\n",
    "    \n",
    "    # get the mean of ref, query, read lengths and quality\n",
    "    tmp = insertions_df.groupby(by=['chr', 'pos', 'strand +'], as_index=False, dropna=False).mean(numeric_only=True)\n",
    "    res_df = tmp1.merge(tmp, on=['chr', 'pos', 'strand +'])\n",
    "    \n",
    "    return res_df\n",
    "\n",
    "\n",
    "bam_irl = 'toy-data/2020_SB-bam/B16-1_1-RT_IRL.bam'\n",
    "bam_irl_df = process_bam(bam_irl, refseq2chr, False)\n",
    "display(bam_irl_df)\n",
    "\n",
    "\n",
    "bam_irr = 'toy-data/2020_SB-bam/B16-1_1-RT_IRR.bam'\n",
    "bam_irr_df = process_bam(bam_irr, refseq2chr, True)\n",
    "display(bam_irr_df)\n",
    "\n",
    "\n",
    "print(bam_irl_df['count'].sum())\n",
    "print(bam_irr_df['count'].sum())\n",
    "print(bam_irl_df.equals(bam_irr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# GRCm39 \n",
    "# https://www.ncbi.nlm.nih.gov/assembly/GCF_000001635.27/\n",
    "# https://genome.ucsc.edu/cgi-bin/hgTracks?chromInfoPage=&hgsid=1560703641_1YwiSDzyFEZ8nuDrTobTnwtYvReT\n",
    "\n",
    "refseq2chr = {\n",
    "    'NC_000067.7': 'chr1',\n",
    "    'NC_000068.8': 'chr2',\n",
    "    'NC_000069.7': 'chr3',\n",
    "    'NC_000070.7': 'chr4',\n",
    "    'NC_000071.7': 'chr5',\n",
    "    'NC_000072.7': 'chr6',\n",
    "    'NC_000073.7': 'chr7',\n",
    "    'NC_000074.7': 'chr8',\n",
    "    'NC_000075.7': 'chr9',\n",
    "    'NC_000076.7': 'chr10',\n",
    "    'NC_000077.7': 'chr11',\n",
    "    'NC_000078.7': 'chr12',\n",
    "    'NC_000079.7': 'chr13',\n",
    "    'NC_000080.7': 'chr14',\n",
    "    'NC_000081.7': 'chr15',\n",
    "    'NC_000082.7': 'chr16',\n",
    "    'NC_000083.7': 'chr17',\n",
    "    'NC_000084.7': 'chr18',\n",
    "    'NC_000085.7': 'chr19',\n",
    "    'NC_000086.8': 'chrX',\n",
    "    'NC_000087.8': 'chrY',\n",
    "    'NC_005089.1': 'chrM'\n",
    "}\n",
    "\n",
    "# record the insertions stats (direction, +/-, and all that)\n",
    "def get_insertion_properties(insertion, chrdict):\n",
    "    res = {\n",
    "        'name': [insertion.query_name],\n",
    "        'chr':[chrdict[insertion.reference_name]],\n",
    "        'pos': [insertion.reference_start],  # 0-based left most coordinate\n",
    "        'strand +': [insertion.is_forward],\n",
    "        'ref length': [insertion.reference_length],\n",
    "        'query length': [insertion.infer_query_length()],  # does not include hard-clipped bases\n",
    "        'read length': [insertion.infer_read_length()],  # does include hard-clipped bases. should be equal to len(query_sequence)\n",
    "        'mapping quality': [insertion.mapping_quality],\n",
    "    }\n",
    "    res = pd.DataFrame.from_dict(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_bam(bam_file, chr_dict, is_irr):\n",
    "    bam = pysam.AlignmentFile(bam_file)\n",
    "    insertions = []\n",
    "    for i, read1 in enumerate(samfile.fetch()):\n",
    "        \n",
    "        # don't even bother with reads that aren't paired (though this shouldn't ever happen, doens't hurt to be safe)\n",
    "        if not read1.is_paired:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        # only look at read 1\n",
    "        if read1.is_read1:\n",
    "            # if read 1 is mapped, continue with this read\n",
    "            if read1.is_mapped:\n",
    "                read = read1\n",
    "            # else read 1 isn't mapped, check if the mate (read 2) is mapped and use that read\n",
    "            elif read1.mate_is_mapped:\n",
    "                read = samfile.mate(read1)\n",
    "            # if neither are mapped, then we won't process these paired reads\n",
    "            else:\n",
    "                continue\n",
    "        # skip read 2 so that we aren't doubling our insertions\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # check if the contig (chromosome) is what we want\n",
    "        if read.reference_name not in chr_dict.keys():\n",
    "            continue\n",
    "        \n",
    "        # check if read is forward (+) or reverse (-) and if it's IRR/IRL\n",
    "        if read.is_forward:\n",
    "            if is_irr:  # +, IRR, then starts with TA\n",
    "                if read.get_forward_sequence()[:2] == 'TA':\n",
    "                    insert_properties = get_insertion_properties(read, chr_dict)\n",
    "                # if not, check if the mate read is mapped and see if that read has TA\n",
    "                elif read.mate_is_mapped:\n",
    "                    mate_read = samfile.mate(read)\n",
    "                    if mate_read.get_forward_sequence()[:2] == 'TA':\n",
    "                        insert_properties = get_insertion_properties(mate_read, chr_dict)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:  # +, IRL, then ends with TA\n",
    "                if read.get_forward_sequence()[:-2] == 'TA':\n",
    "                    insert_properties = get_insertion_properties(read, chr_dict)\n",
    "                # if not, check if the mate read is mapped and see if that read has TA\n",
    "                elif read.mate_is_mapped:\n",
    "                    mate_read = samfile.mate(read)\n",
    "                    if mate_read.get_forward_sequence()[:-2] == 'TA':\n",
    "                        insert_properties = get_insertion_properties(mate_read, chr_dict)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            if is_irr:  # -, IRR, then ends with TA\n",
    "                if read.get_forward_sequence()[:-2] == 'TA':\n",
    "                    insert_properties = get_insertion_properties(read, chr_dict)\n",
    "                # if not, check if the mate read is mapped and see if that read has TA\n",
    "                elif read.mate_is_mapped:\n",
    "                    mate_read = samfile.mate(read)\n",
    "                    if mate_read.get_forward_sequence()[:-2] == 'TA':\n",
    "                        insert_properties = get_insertion_properties(mate_read, chr_dict)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:  # -, IRL, then starts with TA\n",
    "                if read.get_forward_sequence()[:2] == 'TA':\n",
    "                    insert_properties = get_insertion_properties(read, chr_dict)\n",
    "                # if not, check if the mate read is mapped and see if that read has TA\n",
    "                elif read.mate_is_mapped:\n",
    "                    mate_read = samfile.mate(read)\n",
    "                    if mate_read.get_forward_sequence()[:2] == 'TA':\n",
    "                        insert_properties = get_insertion_properties(mate_read, chr_dict)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # record this as an insertion at a site\n",
    "        insertions.append(insert_properties)\n",
    "        \n",
    "    insertions_df = pd.concat(insertions, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # fix strand orientation depending on sequencing library\n",
    "    # For IRR: + if forward, - if not. For IRL this is reversed\n",
    "    if is_irr:\n",
    "        insertions_df['seq library'] = 'IRR'\n",
    "        insertions_df['promoter orient +'] = insertions_df['strand +']\n",
    "    else:\n",
    "        insertions_df['seq library'] = 'IRL'\n",
    "        insertions_df['promoter orient +'] = ~insertions_df['strand +']\n",
    "    \n",
    "    # make the orientations easier to read (+/-)\n",
    "    insertions_df['strand'] = np.where(insertions_df['strand +'], '+', '-')\n",
    "    insertions_df['promoter orient'] = np.where(insertions_df['promoter orient +'], '+', '-')\n",
    "    insertions_df = insertions_df.drop(['strand +', 'promoter orient +'], axis=1)\n",
    "    \n",
    "    \n",
    "    # group together reads that occur at the same chr, pos, and strand. Get the counts.\n",
    "    # TODO: should we be grouping at the strand orientation or the promotoer orientation? \n",
    "    # TODO: However in this case, we are only looking at just IRL or just IRR\n",
    "    # TODO: therefore I can't see the possibilty that an inseriton can occur in the same palce in both directions\n",
    "    # TODO: though theoretically possible with distinct clonal expansion...?\n",
    "    group_cols = ['chr', 'pos', 'strand', 'promoter orient', 'seq library']\n",
    "    \n",
    "    tmp = insertions_df.groupby(by=group_cols, as_index=False, dropna=False)['name'].count()\n",
    "    tmp1 = tmp[group_cols]\n",
    "    tmp1['count'] = tmp[['name']]\n",
    "    \n",
    "    # get the mean of ref, query, read lengths and quality?\n",
    "    tmp = insertions_df.groupby(by=group_cols, as_index=False, dropna=False).mean(numeric_only=True)\n",
    "    res_df = tmp1.merge(tmp, on=group_cols)\n",
    "    \n",
    "    return res_df\n",
    "\n",
    "\n",
    "bam_irl = 'toy-data/2020_SB-bam/B16-1_1-RT_IRL.bam'\n",
    "inserts_irl_df = process_bam(bam_irl, refseq2chr, False)\n",
    "\n",
    "bam_irr = 'toy-data/2020_SB-bam/B16-1_1-RT_IRR.bam'\n",
    "inserts_irr_df = process_bam(bam_irr, refseq2chr, True)\n",
    "\n",
    "inserts_df = pd.concat([inserts_irl_df, inserts_irr_df], ignore_index=True)\n",
    "inserts_df = inserts_df.sort_values(['chr', 'pos'], ignore_index=True)\n",
    "inserts_df['counts_irr'] = np.where(inserts_df['seq library'] == 'IRR', inserts_df['count'], 0)\n",
    "inserts_df['counts_irl'] = np.where(inserts_df['seq library'] == 'IRL', inserts_df['count'], 0)\n",
    "inserts_df = inserts_df[['chr', 'pos', 'promoter orient', 'counts_irr', 'counts_irl']]\n",
    "\n",
    "display(inserts_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a col for promoter is on + strand or not\n",
    "# promoter faces IRR, so IRR is + then so is promoter. maybe make this \"transposon orientation, or tnp orient\" and change to +/-\n",
    "# and for the transposon IRL/IRR call it sequence library, maybe seq lib\n",
    "\n",
    "# TODO: look at Aak1 in my research. do I have it? is it in the snps? is it in the pathways? where can it be found in the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = inserts_df[['chr', 'pos', 'strand', 'promoter orient', 'seq library', 'counts_irr', 'counts_irl']]\n",
    "display(tmp.groupby(by=['chr', 'pos', 'promoter orient'], as_index=False, dropna=False).sum(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_promoter = inserts_df.groupby(by=['chr', 'pos', 'promoter orient'], as_index=False, dropna=False).sum(numeric_only=True)\n",
    "display(tmp_promoter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_ind = inserts_irl_df['chr'] + '-' + inserts_irl_df['pos'].astype(str)\n",
    "irr_ind = inserts_irr_df['chr'] + '-' + inserts_irr_df['pos'].astype(str)\n",
    "\n",
    "np.intersect1d(irl_ind, irr_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inserts_df['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inserts_df['pos']))\n",
    "print(len(np.unique(inserts_df['pos'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check IRL and IRR. Why do they have the same insertons places and counts if we don't consider IRL and IRR?\n",
    "# correct IRL strand orientation and collapse the IRL/IRR designation into just the insertion location\n",
    "# make two columns, each is the IRL or IRR count (0 if not present, but this will appear as NaN) and then whatever is decided (take the max of the two) can be done down the line\n",
    "tmp = inserts_df\n",
    "\n",
    "tmpirr = tmp[tmp['seq library'] == 'IRR'].drop(['seq library', 'strand', 'promoter orient', 'ref length', 'query length', 'read length', 'mapping quality'], axis=1)\n",
    "tmpirr.rename(columns={'count': 'count_irr'}, inplace=True)\n",
    "\n",
    "tmpirl = tmp[tmp['seq library'] == 'IRL'].drop(['seq library', 'strand', 'promoter orient', 'ref length', 'query length', 'read length', 'mapping quality'], axis=1)\n",
    "tmpirl.rename(columns={'count': 'count_irl'}, inplace=True)\n",
    "\n",
    "tmp1 = pd.concat([tmpirr, tmpirl], ignore_index=True)\n",
    "tmp1 = tmp1.fillna(0)\n",
    "tmp1['count_irr'] = tmp1['count_irr'].astype(int)\n",
    "tmp1['count_irl'] = tmp1['count_irl'].astype(int)\n",
    "tmp1 = tmp1.sort_values(['chr', 'pos'], ignore_index=True)\n",
    "print(len(tmp1))\n",
    "tmp2 = tmp1.groupby(by=['chr', 'pos'], as_index=False, dropna=False).sum(numeric_only=True)\n",
    "print(len(tmp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, counts = np.unique(inserts_df['pos'], return_counts=True)\n",
    "counts = np.array(counts)\n",
    "dups = positions[counts != 1]\n",
    "\n",
    "display(tmp2[tmp2['pos'].isin(dups)].sort_values('pos'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pysam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# changing chromosome names\n",
    "# GRCm39 https://www.ncbi.nlm.nih.gov/assembly/GCF_000001635.27/\n",
    "# https://genome.ucsc.edu/cgi-bin/hgTracks?chromInfoPage=&hgsid=1560703641_1YwiSDzyFEZ8nuDrTobTnwtYvReT\n",
    "chr_dict = {\n",
    "    'NC_000067.7': 'chr1',\n",
    "    'NC_000068.8': 'chr2',\n",
    "    'NC_000069.7': 'chr3',\n",
    "    'NC_000070.7': 'chr4',\n",
    "    'NC_000071.7': 'chr5',\n",
    "    'NC_000072.7': 'chr6',\n",
    "    'NC_000073.7': 'chr7',\n",
    "    'NC_000074.7': 'chr8',\n",
    "    'NC_000075.7': 'chr9',\n",
    "    'NC_000076.7': 'chr10',\n",
    "    'NC_000077.7': 'chr11',\n",
    "    'NC_000078.7': 'chr12',\n",
    "    'NC_000079.7': 'chr13',\n",
    "    'NC_000080.7': 'chr14',\n",
    "    'NC_000081.7': 'chr15',\n",
    "    'NC_000082.7': 'chr16',\n",
    "    'NC_000083.7': 'chr17',\n",
    "    'NC_000084.7': 'chr18',\n",
    "    'NC_000085.7': 'chr19',\n",
    "    'NC_000086.8': 'chrX',\n",
    "    'NC_000087.8': 'chrY',\n",
    "    'NC_005089.1': 'chrM'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insertion_properties(insertion, chrdict):\n",
    "    # record the insertions stats (direction, +/-, and all that)\n",
    "    # NOTE: here is where additional statistics and or properties for each insertion site can be added\n",
    "    res = {\n",
    "        'name': [insertion.query_name],\n",
    "        'chr':[chrdict[insertion.reference_name]],\n",
    "        'pos': [insertion.reference_start],  # 0-based left most coordinate\n",
    "        'strand +': [insertion.is_forward],\n",
    "        'ref length': [insertion.reference_length],\n",
    "        'query length': [insertion.infer_query_length()],  # does not include hard-clipped bases\n",
    "        'read length': [insertion.infer_read_length()],  # does include hard-clipped bases. should be equal to len(query_sequence)\n",
    "        'mapping quality': [insertion.mapping_quality],  # MAPQ: MAPping Quality.\n",
    "        # MAPQ equals −10 log10 Pr{mapping position is wrong}, rounded to the nearest integer.\n",
    "        # A value 255 indicates that the mapping quality is not available.\n",
    "        # otherwise, the higher the number, the more confident of the quality of the mapping\n",
    "        # see solution for x in wolfram\n",
    "        #       254 = -10 * log10(x) \n",
    "        #       11 = -10 * log10(x)\n",
    "    }\n",
    "    res = pd.DataFrame.from_dict(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_is_quality(read, is_irr, chr_dict):\n",
    "    # that is paired\n",
    "    if not read.is_paired:\n",
    "        return False\n",
    "    \n",
    "    # this is mapped\n",
    "    if not read.is_mapped:\n",
    "        return False\n",
    "\n",
    "    # and has a contig (chromosome) is the predefined dict\n",
    "    if read.reference_name not in chr_dict.keys():\n",
    "        return False\n",
    "\n",
    "    # check if read is forward (+) or reverse (-), then see if 'TA' is present with respects to IRR/IRL orientation\n",
    "    if read.is_forward:\n",
    "        if is_irr:  # +, IRR, then starts with TA\n",
    "            if read.get_forward_sequence()[:2] == 'TA':\n",
    "                return True\n",
    "        else:  # +, IRL, then ends with TA\n",
    "            if read.get_forward_sequence()[:-2] == 'TA':\n",
    "                return True\n",
    "    else:\n",
    "        if is_irr:  # -, IRR, then ends with TA\n",
    "            if read.get_forward_sequence()[:-2] == 'TA':\n",
    "                return True\n",
    "        else:  # -, IRL, then starts with TA\n",
    "            if read.get_forward_sequence()[:2] == 'TA':\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def process_bam(file, chr_dict, is_irr):\n",
    "    bam = pysam.AlignmentFile(file, 'rb')\n",
    "    insertions = []\n",
    "    for read1 in bam.fetch():  # multiple_iterators=True\n",
    "        # only look at read 1\n",
    "        if not read1.is_read1:\n",
    "            continue\n",
    "        # if the read1 is a quality read, then get the insertions properties\n",
    "        if read_is_quality(read1, is_irr, chr_dict):\n",
    "            insert_properties = get_insertion_properties(read1, chr_dict)\n",
    "            insertions.append(insert_properties)\n",
    "        # check if read 2 (the mate read) is quality and can be used for insertion properties\n",
    "        else:\n",
    "            # must have a mate read that is mapped for .mate() to return properly\n",
    "            if read1.mate_is_unmapped or (not read1.is_paired):\n",
    "                continue\n",
    "            \n",
    "            read2 = bam.mate(read1)\n",
    "\n",
    "            # also check if read2 and read1 mapped to the same reference_name\n",
    "            if read1.reference_name != read2.reference_name:\n",
    "                continue\n",
    "\n",
    "            # then check if the read2 is a quality read and get the insertion properties\n",
    "            if read_is_quality(read2, is_irr, chr_dict):\n",
    "                insert_properties = get_insertion_properties(read2, chr_dict)\n",
    "                insertions.append(insert_properties)\n",
    "\n",
    "    bam.close()\n",
    "    # check if there were any inseritons at all to avoid errors from pandas.concat()\n",
    "    if len(insertions) == 0:\n",
    "        return None\n",
    "    \n",
    "    insertions_df = pd.concat(insertions, axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: put everything below in a separate function and return insertions_df\n",
    "    # set transposon promoter orientation depending on sequencing library\n",
    "    # For IRR: + if forward, - if not. For IRL this is reversed\n",
    "    # using 'strand +' as a cool is easy to change, but it could also have been set this way in get_insertion_properties()\n",
    "    if is_irr:\n",
    "        insertions_df['seq library'] = 'IRR'\n",
    "        insertions_df['tpn promoter orient +'] = insertions_df['strand +']\n",
    "    else:\n",
    "        insertions_df['seq library'] = 'IRL'\n",
    "        insertions_df['tpn promoter orient +'] = ~insertions_df['strand +']\n",
    "    \n",
    "    # make the orientations easier to read (+/-)\n",
    "    insertions_df['strand'] = np.where(insertions_df['strand +'], '+', '-')\n",
    "    insertions_df['tpn promoter orient'] = np.where(insertions_df['tpn promoter orient +'], '+', '-')\n",
    "    insertions_df = insertions_df.drop(['strand +', 'tpn promoter orient +'], axis=1)\n",
    "    \n",
    "    # Get the counts by grouping reads that occur at the same chr, pos, strand, tpn promotoer orient, and seq library.\n",
    "    group_cols = ['chr', 'pos', 'strand', 'tpn promoter orient', 'seq library']\n",
    "    tmp = insertions_df.groupby(by=group_cols, as_index=False, dropna=False)['name'].count()\n",
    "    tmp1 = tmp[group_cols]\n",
    "    tmp1['count'] = tmp[['name']]\n",
    "    # keep track of individual read names to ensure uniqueness of insertion sites\n",
    "    tmp1['read names'] = insertions_df.groupby(by=group_cols, dropna=False)['name'].apply(list).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # get the mean of ref, query, read lengths and quality\n",
    "    tmp2 = insertions_df.groupby(by=group_cols, as_index=False, dropna=False).mean(numeric_only=True)\n",
    "    # change the column names to reflect the mean\n",
    "    tmp2 = tmp2.rename({'ref length': 'ref length (mean)', 'query length': 'query length (mean)', 'read length': 'read length (mean)',\n",
    "                      'mapping quality': 'mapping quality (mean)'}, axis=1)\n",
    "    \n",
    "    # also get median and stdev for mapping quality\n",
    "    tmp2['mapping quality (median)'] = insertions_df.groupby(by=group_cols, as_index=False, dropna=False).median(numeric_only=True)['mapping quality']\n",
    "    tmp2['mapping quality (stdev)'] = insertions_df.groupby(by=group_cols, as_index=False, dropna=False).std(numeric_only=True)['mapping quality']\n",
    "\n",
    "    res_df = tmp1.merge(tmp2, on=group_cols)\n",
    "    tmp_names = res_df.pop('read names')\n",
    "    res_df.insert(len(res_df.columns.values), 'read names', tmp_names)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that encountered errors\n",
    "# EL4-36_3-LT           \n",
    "# B16PD1-531_2-LT       \n",
    "# B16PD1-534_1-RT       \n",
    "# B16-475_2-LT          \n",
    "# B16-485_3-S           \n",
    "# B16-516_2-RT          \n",
    "# EL4-47_1-RT           \n",
    "\n",
    "sample = \"EL4-36_3-LT\"\n",
    "data_dir = Path(\"/project/cs-myers/MathewF/projects/Laura-SB-Analysis/data/2020_SB-bam\")\n",
    "irl_bam = data_dir / f\"{sample}_IRL.bam\"\n",
    "irr_bam = data_dir / f\"{sample}_IRR.bam\"\n",
    "\n",
    "\n",
    "# resolve IRR and IRL files and convert them to single insertion site format\n",
    "inserts_irl_df = process_bam(file=irl_bam, chr_dict=chr_dict, is_irr=False)\n",
    "if inserts_irl_df is not None:  # if no insertions present, process_bam returns None\n",
    "    inserts_irl_df['seq library'] = 'IRL'\n",
    "\n",
    "inserts_irr_df = process_bam(file=irr_bam, chr_dict=chr_dict, is_irr=True)\n",
    "if inserts_irr_df is not None:  # if no insertions present, process_bam returns None\n",
    "    inserts_irr_df['seq library'] = 'IRR'\n",
    "\n",
    "# concat of a dataframe and None just results in the original dataframe\n",
    "inserts_df = pd.concat([inserts_irl_df, inserts_irr_df], ignore_index=True)\n",
    "inserts_df = inserts_df.sort_values(['chr', 'pos'], ignore_index=True)\n",
    "\n",
    "# get seq library specific counts\n",
    "count_irr = np.where(inserts_df['seq library'] == 'IRR', inserts_df['count'], 0)\n",
    "count_irl = np.where(inserts_df['seq library'] == 'IRL', inserts_df['count'], 0)\n",
    "inserts_df.insert(6, \"count_irr\", count_irr)\n",
    "inserts_df.insert(7, \"count_irl\", count_irl)\n",
    "tmp_read_name = inserts_df.pop('read names')\n",
    "inserts_df.insert(len(inserts_df.columns.values), \"read names\", tmp_read_name)\n",
    "\n",
    "# verify that insertions did not count both read1 and read2\n",
    "# do this by checking that the length of 'read names'is the same number as the length of unique read names\n",
    "read_names = inserts_df['read names'].to_numpy()\n",
    "for i in range(len(read_names)):\n",
    "    sample = read_names[i]\n",
    "    assert len(np.unique(sample)) == len(sample)\n",
    "        \n",
    "# show total irr and irl conuts\n",
    "print(f\"irr insertions: {inserts_df['count_irr'].sum()}\")\n",
    "print(f\"irl insertions: {inserts_df['count_irl'].sum()}\")\n",
    "\n",
    "display(inserts_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from typing import Generator\n",
    "\n",
    "from docopt import docopt\n",
    "import numpy as np\n",
    "from pandas import read_csv, concat, DataFrame\n",
    "import networkx as nx\n",
    "\n",
    "# from importlib import reload\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'insertion_dir': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/data/2020_SB-insertions'),\n",
    "    'output': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/data/2020_SB-graphs'),\n",
    "    \n",
    "    # 'insertion_dir': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/NetCIS/toy-data/2020_SB-insertions/'),\n",
    "    # 'output': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/NetCIS/toy-data/2020_SB-graphs/'),\n",
    "    \n",
    "    'verbose': 1,\n",
    "    'jobs': 8,\n",
    "    'threshold': 50000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output\n",
    "out_dir_case = args['output'] / \"case\"\n",
    "out_dir_case.mkdir(parents=True, exist_ok=True)\n",
    "out_dir_control = args['output'] / \"control\"\n",
    "out_dir_control.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in data dir, load each file as pandas.DataFrame, and add meta data based on the file name\n",
    "insert_list = []\n",
    "for file in args[\"insertion_dir\"].iterdir():\n",
    "    cell_type, cell_id, tumor_type = file.stem.split(\"-\")\n",
    "    tmp_df = read_csv(file)\n",
    "    tmp_df[\"cell type\"] = cell_type\n",
    "    tmp_df[\"cell id\"] = cell_id\n",
    "    tmp_df[\"tumor type\"] = tumor_type\n",
    "    insert_list.append(tmp_df)\n",
    "inserts_df = concat(insert_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into case/controls\n",
    "insert_case = inserts_df[inserts_df[\"tumor type\"] == \"S\"]\n",
    "insert_control = inserts_df[inserts_df[\"tumor type\"] != \"S\"]\n",
    "\n",
    "# get all chromosomes to separate further the case/controls dataframes\n",
    "chrom_list = np.unique(inserts_df[\"chr\"].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some instances where insertions occur both in irl and irr, but to the extent that they occur in equal amounts is not clear\n",
    "\n",
    "This should be further investigated in what irl and irr have these odd properties together. See investigate_irr_irl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both_libs = insertion_nodes[ (insertion_nodes['count_irl'] != 0) & (insertion_nodes['count_irr'] != 0) ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph (network) of insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(chrom_df: DataFrame, threshold, save_file, verbose=0) -> None:\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # prepare the insertions by grouping them together\n",
    "    # find the total count of insertions and the counts per sequencing library (IRR/IRL)\n",
    "    insert_cols = ['chr', 'pos', 'tpn promoter orient', 'seq library']\n",
    "    tmp = chrom_df.groupby(by=insert_cols, as_index=False, dropna=False)['read name'].count()\n",
    "    tmp['count'] = tmp.pop('read name')\n",
    "    count_irr = np.where(tmp['seq library'] == 'IRR', tmp['count'], 0)\n",
    "    count_irl = np.where(tmp['seq library'] == 'IRL', tmp['count'], 0)\n",
    "    tmp.insert(5, \"count_irr\", count_irr)\n",
    "    tmp.insert(6, \"count_irl\", count_irl)\n",
    "    \n",
    "    # group insertions without the sequencing library. \n",
    "    # As long as the transposon orientation, chromosome, and position are the same, \n",
    "    # then it does not matter which library the insertion came from\n",
    "    node_cols = ['chr', 'pos', 'tpn promoter orient']\n",
    "    insertion_nodes = tmp.groupby(by=node_cols, as_index=False, dropna=False).sum(numeric_only=True)\n",
    "    insertion_nodes['read names'] = chrom_df.groupby(by=node_cols, dropna=False, group_keys=False)['read name'].apply(list).reset_index(drop=True)\n",
    "    \n",
    "    # TODO: for some reason there are few insertions that occur both in IRR and IRL\n",
    "    # both_libs = insertion_nodes[ (insertion_nodes['count_irl'] != 0) & (insertion_nodes['count_irr'] != 0) ]\n",
    "    \n",
    "    # add each insertion. Since they are unique, I can add the edges after all the nodes are in\n",
    "    for i in range(len(insertion_nodes)):\n",
    "        if (i % 1000 == 0) and (i != 0) and verbose:\n",
    "            print(f\"\\t{i+1/len(insertion_nodes)} insertions\")\n",
    "        insert = insertion_nodes.iloc[i]\n",
    "        new_node = f\"{insert['pos']}|{insert['tpn promoter orient']}\"\n",
    "        # add node(i) as an insertion location into the network\n",
    "        G.add_node(\n",
    "            new_node,\n",
    "            counts=insert[\"count\"],\n",
    "            counts_irr=insert[\"count_irr\"],\n",
    "            counts_irl=insert[\"count_irl\"],\n",
    "            orient=insert[\"tpn promoter orient\"],\n",
    "            chrom=insert[\"chr\"],\n",
    "            position=insert[\"pos\"],\n",
    "        )\n",
    "        # for other_node in G.nodes:\n",
    "        #     if other_node == new_node:\n",
    "        #         continue\n",
    "        #     # find distance between nodes using their position\n",
    "        #     node_dist = abs(G.nodes[other_node][\"position\"] - G.nodes[new_node][\"position\"])\n",
    "        #     # double check don't add edge to self\n",
    "        #     if node_dist == 0:\n",
    "        #         continue\n",
    "        #     # if distance between node(i) and node(j) is less than threshold\n",
    "        #     if node_dist <= threshold:\n",
    "        #         # add edge(ij) with a weight of the distance (or inverse?) to the network\n",
    "        #         \n",
    "        #         G.add_edge(new_node, other_node, weight=1 / node_dist)\n",
    "    \n",
    "    # the following code does what is commented out above but I am keeping all of this in for a future user to reference\n",
    "    \n",
    "    # nodes are inherently ordered as they are added in the graph. \n",
    "    # however, the ordering doens't have to numerically make sense\n",
    "    ordered_nodes = G.nodes()\n",
    "    # remove the transposon orientation from the end of the node name\n",
    "    tmp_order = [ int(x.split(\"|\")[0]) for x in ordered_nodes ]\n",
    "    # check if this changes the number of unique nodes.\n",
    "    # If we have + and - at the same location, this assert will fail.\n",
    "    # This isn't a bad thing but I want to know when it is happening\n",
    "    assert len(np.unique(ordered_nodes)) == len(np.unique(tmp_order))\n",
    "    \n",
    "    # cast the nodes into a numpy array that can be used to broadcast into a symmetric matrix of distances\n",
    "    nodes = np.array(tmp_order).reshape(-1, 1)\n",
    "    dist_nodes = np.abs(nodes - nodes.T)  # symmetric 2d array\n",
    "    \n",
    "    # cis nodes are those that are under the threshold\n",
    "    cis_nodes = dist_nodes < threshold  # symmetric 2d array\n",
    "    \n",
    "    # get the indices of the lower left triangle of the symmetric matrix.\n",
    "    # edges_ind is a tuple of two array. The same index location in both arrays is used \n",
    "    # to index a single value from the symmetric matrix. This results in two very long \n",
    "    # arrays that will index all the values of the lower left triangle of the matrix\n",
    "    edges_ind = np.tril_indices_from(cis_nodes, k=-1) # tuple of two 1d arrays\n",
    "    \n",
    "    # keep nodes that are under the threshold\n",
    "    keep_nodes = cis_nodes[edges_ind]  # 1d array\n",
    "    \n",
    "    # set up the nodes to be a numpy array for easy indexing\n",
    "    ordered_nodes = np.array(G.nodes())  # 1d array\n",
    "    \n",
    "    # get the actual node names for the lower left triangle via as the column\n",
    "    nodes1 = ordered_nodes[edges_ind[1][keep_nodes]]  # 1d array\n",
    "    # the rows\n",
    "    nodes2 = ordered_nodes[edges_ind[0][keep_nodes]]  # 1d array\n",
    "    # and edge weights (TODO: which can be modified for a differnt weighting method, maybe 1 / log10(x) instead?)\n",
    "    nodes_dist = 1 / dist_nodes[edges_ind][keep_nodes]  # 1d array\n",
    "    # combine the nodes and weights into an iterable that can be passed wholly into the graph\n",
    "    # an edge is defined as the first node, the second node, and then a dict of attributes, such as weight\n",
    "    edges_to_add = [ (x, y, {\"weight\": z}) for x, y, z in zip(nodes1, nodes2, nodes_dist) ]\n",
    "    G.add_edges_from(edges_to_add)\n",
    "\n",
    "    # save the graph\n",
    "    nx.write_graphml(G, save_file)\n",
    "\n",
    "def create_graph_helper(iter_args) -> None:\n",
    "    insert_case_chrom, insert_control_chrom, threshold, case_file, control_file = iter_args\n",
    "    create_graph(insert_case_chrom, threshold, case_file)\n",
    "    create_graph(insert_control_chrom, threshold, control_file)\n",
    "    \n",
    "def create_graph_generator(chrom_list, insert_case, insert_control, threshold, case_dir, control_dir) -> Generator[tuple, None, None]:\n",
    "    for chrom in chrom_list:\n",
    "        print(chrom)\n",
    "        insert_case_chrom = insert_case[insert_case['chr'] == chrom]    \n",
    "        insert_control_chrom = insert_control[insert_control['chr'] == chrom]\n",
    "        case_file = case_dir / f\"{chrom}.graphml\"\n",
    "        control_file = control_dir / f\"{chrom}.graphml\"\n",
    "        yield ( insert_case_chrom, insert_control_chrom, threshold, case_file, control_file )\n",
    "        \n",
    "create_graph(insert_case[insert_case['chr'] == \"chr1\"] , 50000, out_dir_case / \"chr1.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_gen = create_graph_generator(chrom_list, insert_case, insert_control, args['threshold'], out_dir_case, out_dir_control)\n",
    "with Pool(args[\"jobs\"]) as p:\n",
    "    [ x for x in p.imap_unordered(create_graph_helper, iter_gen) ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_properties(G):\n",
    "    print(f\"number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"number of edges: {G.number_of_edges()}\")\n",
    "    num_inserts = 0\n",
    "    for node in G.nodes:\n",
    "        num_inserts += G.nodes[node]['counts']\n",
    "    print(f\"number of insertions: {num_inserts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml(args['output'] / 'case-chr1.graphml')\n",
    "graph_properties(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs_by_nodes = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "num_subgraphs = len(subgraphs_by_nodes)\n",
    "num_subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_degrees = [ len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) ]\n",
    "(\n",
    "    so.Plot(x=subgraph_degrees)\n",
    "    .add(so.Bars(), so.Hist(bins=50))\n",
    "    .scale(x=\"symlog\")\n",
    "    .label(x='subgraph degree', y='count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = [ G.nodes[node]['counts'] for node in list(subgraphs_by_nodes[0]) ]\n",
    "(\n",
    "    so.Plot(y=node_size, x=list(subgraphs_by_nodes[0]))\n",
    "    .add(so.Bars())\n",
    "    .label(x='position', y='# of insertions')\n",
    "    .layout(size=(12, 4))\n",
    ")\n",
    "\n",
    "# TODO: check if the nodes are sorted by name (genomic position) \n",
    "# maybe make pandas DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_subgraph = G.subgraph(subgraphs_by_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n",
    "pos = nx.kamada_kawai_layout(tmp_subgraph)\n",
    "node_size = [ G.nodes[node]['counts'] for node in list(subgraphs_by_nodes[0]) ]\n",
    "nx.draw_networkx(tmp_subgraph, with_labels=False, pos=pos, node_size=node_size, alpha=0.5, linewidths=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(tmp_subgraph)\n",
    "for node, [x, y] in pos.items():\n",
    "    pos[node] = [node, y]\n",
    "node_size = [ G.nodes[node]['counts'] for node in list(subgraphs_by_nodes[0]) ]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "nx.draw_networkx(tmp_subgraph, with_labels=False, pos=pos, node_size=node_size, alpha=0.5, linewidths=0.5, ax=ax)\n",
    "ticks = np.arange(min(pos.keys())-10000, max(pos.keys())+1000, 10000)\n",
    "# ax.set_xticks(ticks)\n",
    "ax.grid(axis='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "test = nx.nx_agraph.to_agraph(tmp_subgraph)\n",
    "test.layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze subgraphs\n",
    "\n",
    "# histogram of subgraph orders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "data_dir = Path(\"toy-data/2020_SB-fastq\")\n",
    "output_prefix = \"toy-data/2020_SB\"\n",
    "genome_index_dir = \"/project/cs-myers/MathewF/software/bowtie2-2.4.5/indexes/GRCm39/GRCm39\"\n",
    "N = 8\n",
    "input_files = \"toy-data/input.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.read_csv(input_files, sep=\"\\t\", header=None)\n",
    "display(files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_output_dir = Path(output_prefix + \"-bam\")\n",
    "bam_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "insertions_output_dir = Path(output_prefix + \"-insertions\")\n",
    "insertions_output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use multiprocessing here. No additional threads for bowtie2\n",
    "for row in files_df.iterrows():\n",
    "    row = row[1]\n",
    "    mysample = row[0]\n",
    "    irl_F = data_dir / row[1]\n",
    "    irl_R = data_dir / row[2]\n",
    "    irr_F = data_dir / row[3]\n",
    "    irr_R = data_dir / row[4]\n",
    "    \n",
    "    # Process IRL reads ---> trim adaptor ---> map reads\n",
    "    # append temp names to the files\n",
    "    trim_irl_f1 = data_dir / (\n",
    "        irl_F.name.rstrip(\"\".join(irl_F.suffixes)) + \"-5trim\" + \"\".join(irl_F.suffixes)\n",
    "    )\n",
    "    trim_irl_r1 = data_dir / (\n",
    "        irl_R.name.rstrip(\"\".join(irl_R.suffixes)) + \"-5trim\" + \"\".join(irl_R.suffixes)\n",
    "    )\n",
    "\n",
    "    trim_irl_f2 = data_dir / (\n",
    "        irl_F.name.rstrip(\"\".join(irl_F.suffixes)) + \"-53trim\" + \"\".join(irl_F.suffixes)\n",
    "    )\n",
    "    trim_irl_r2 = data_dir / (\n",
    "        irl_R.name.rstrip(\"\".join(irl_R.suffixes)) + \"-53trim\" + \"\".join(irl_R.suffixes)\n",
    "    )\n",
    "\n",
    "    trim_irl_f3 = data_dir / (\n",
    "        irl_F.name.rstrip(\"\".join(irl_F.suffixes)) + \"-trim\" + \"\".join(irl_F.suffixes)\n",
    "    )\n",
    "    trim_irl_r3 = data_dir / (\n",
    "        irl_R.name.rstrip(\"\".join(irl_R.suffixes)) + \"-trim\" + \"\".join(irl_R.suffixes)\n",
    "    )\n",
    "\n",
    "    irl_sam = bam_output_dir / (mysample + \"_IRL.sam\")\n",
    "    irl_bam = bam_output_dir / (mysample + \"_IRL.bam\")\n",
    "\n",
    "    # https://cutadapt.readthedocs.io/en/stable/guide.html#id4\n",
    "    # --front or -g\n",
    "    # -G is for read 2\n",
    "    # # -g is found by regular 5': Full adapter sequence anywhere, Partial adapter sequence at 5’ end, Full adapter sequence at 5’ end\n",
    "    # # -g ^ is found by anchored 5': Full adapter sequence at 5’ end\n",
    "    os.system(f\"cutadapt --quiet -j {N} --discard-untrimmed -g GTATGTAAACTTCCGACTTCAACTG -o {trim_irl_f1} -p {trim_irl_r1} {irl_F} {irl_R}\")\n",
    "\n",
    "    os.system(f\"cutadapt --quiet -j {N} -G ^GTAATACGACTCACTATAGGGCTCCGCTTAAGGGAC -o {trim_irl_f2} -p {trim_irl_r2} {trim_irl_f1} {trim_irl_r1}\")\n",
    "    os.system(f\"rm {trim_irl_f1}\")\n",
    "    os.system(f\"rm {trim_irl_r1}\")\n",
    "\n",
    "    # --adapter or -a\n",
    "    # -A is for read 2\n",
    "    # -a is found by regular 3: Full adapter sequence anywhere, Partial adapter sequence at 3’ end, Full adapter sequence at 3’ end\n",
    "    os.system(f\"cutadapt --quiet -j {N} -a GTCCCTTAAGCGGAGCCCTATAGTGAGTCGTATTAC -A CAGTTGAAGTCGGAAGTTTACATAC -o {trim_irl_f3} -p {trim_irl_r3} {trim_irl_f2} {trim_irl_r2}\")\n",
    "    os.system(f\"rm {trim_irl_f2}\")\n",
    "    os.system(f\"rm {trim_irl_r2}\")\n",
    "\n",
    "    os.system(f\"bowtie2 -p {N} --local --quiet -x {genome_index_dir} -q -1 {trim_irl_f3} -2 {trim_irl_r3} -S {irl_sam}\")\n",
    "    os.system(f\"rm {trim_irl_f3}\")\n",
    "    os.system(f\"rm {trim_irl_r3}\")\n",
    "\n",
    "    os.system(f\"samtools sort -@ {N} -m 4G -l 9 -o {irl_bam} {irl_sam}\")\n",
    "    os.system(f\"samtools index -@ {N} {irl_bam}\")\n",
    "    os.system(f\"rm {irl_sam}\")\n",
    "\n",
    "    # Process IRR reads ---> trim adaptor ---> map reads\n",
    "    trim_irr_f1 = data_dir / (\n",
    "        irr_F.name.rstrip(\"\".join(irr_F.suffixes)) + \"-5trim\" + \"\".join(irr_F.suffixes)\n",
    "    )\n",
    "    trim_irr_r1 = data_dir / (\n",
    "        irr_R.name.rstrip(\"\".join(irr_R.suffixes)) + \"-5trim\" + \"\".join(irr_R.suffixes)\n",
    "    )\n",
    "\n",
    "    trim_irr_f2 = data_dir / (\n",
    "        irr_F.name.rstrip(\"\".join(irr_F.suffixes)) + \"-53trim\" + \"\".join(irr_F.suffixes)\n",
    "    )\n",
    "    trim_irr_r2 = data_dir / (\n",
    "        irr_R.name.rstrip(\"\".join(irr_R.suffixes)) + \"-53trim\" + \"\".join(irr_R.suffixes)\n",
    "    )\n",
    "\n",
    "    trim_irr_f3 = data_dir / (\n",
    "        irr_F.name.rstrip(\"\".join(irr_F.suffixes)) + \"-trim\" + \"\".join(irr_F.suffixes)\n",
    "    )\n",
    "    trim_irr_r3 = data_dir / (\n",
    "        irr_R.name.rstrip(\"\".join(irr_R.suffixes)) + \"-trim\" + \"\".join(irr_R.suffixes)\n",
    "    )\n",
    "    irr_sam = bam_output_dir / (mysample + \"_IRR.sam\")\n",
    "    irr_bam = bam_output_dir / (mysample + \"_IRR.bam\")\n",
    "\n",
    "    os.system(f\"cutadapt --quiet -j {N} --discard-untrimmed -g GTATGTAAACTTCCGACTTCAACTG -o {trim_irr_f1} -p {trim_irr_r1} {irr_F} {irr_R}\")\n",
    "\n",
    "    os.system(f\"cutadapt --quiet -j {N} -G ^GTAATACGACTCACTATAGGGCTCCGCTTAAGGGAC -o {trim_irr_f2} -p {trim_irr_r2} {trim_irr_f1} {trim_irr_r1}\")\n",
    "    os.system(f\"rm {trim_irr_f1}\")\n",
    "    os.system(f\"rm {trim_irr_r1}\")\n",
    "\n",
    "    os.system(f\"cutadapt --quiet -j {N} -a GTCCCTTAAGCGGAGCCCTATAGTGAGTCGTATTAC -A CAGTTGAAGTCGGAAGTTTACATAC -o {trim_irr_f3} -p {trim_irr_r3} {trim_irr_f2} {trim_irr_r2}\")\n",
    "    os.system(f\"rm {trim_irr_f2}\")\n",
    "    os.system(f\"rm {trim_irr_r2}\")\n",
    "\n",
    "    os.system(f\"bowtie2 -p {N} --local --quiet -x {genome_index_dir} -q -1 {trim_irr_f3} -2 {trim_irr_r3} -S {irr_sam}\")\n",
    "    os.system(f\"rm {trim_irr_f3}\")\n",
    "    os.system(f\"rm {trim_irr_r3}\")\n",
    "\n",
    "    os.system(f\"samtools sort -@ {N} -m 4G -l 9 -o {irr_bam} {irr_sam}\")\n",
    "    os.system(f\"samtools index -@ {N} {irr_bam}\")\n",
    "    os.system(f\"rm {irr_sam}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netcis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c87b4bd72cfdac273d31f98cea827e2b7dd5d67bbbc55930ba8dfe1335f64a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
