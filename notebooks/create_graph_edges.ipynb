{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step Add All Edges Algorithm\n",
    "\n",
    "This notebook will illustrate the use of numpy vector based methods to speed up the process of identifying and adding edges to a graph when using distance between nodes (for which the nodes are numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from typing import Generator\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "from docopt import docopt\n",
    "import numpy as np\n",
    "from pandas import read_csv, concat, DataFrame\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'insertion_dir': Path(\"/research/labs/immunology/rogerslm/m277102/projects/NetCIS/2020_SB/all_files-insertions/\"),\n",
    "    'output': Path(\"/research/labs/immunology/rogerslm/m277102/projects/NetCIS/2020_SB/all_files-graphs\"),\n",
    "    \n",
    "    # 'insertion_dir': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/NetCIS/toy-data/2020_SB-insertions/'),\n",
    "    # 'output': Path('/project/cs-myers/MathewF/projects/Laura-SB-Analysis/NetCIS/toy-data/2020_SB-graphs/'),\n",
    "    \n",
    "    'verbose': 1,\n",
    "    'jobs': 8,\n",
    "    'threshold': 50000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output\n",
    "out_dir_case = args['output'] / \"case\"\n",
    "out_dir_case.mkdir(parents=True, exist_ok=True)\n",
    "out_dir_control = args['output'] / \"control\"\n",
    "out_dir_control.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in data dir, load each file as pandas.DataFrame, and add meta data based on the file name\n",
    "insert_list = []\n",
    "for file in args[\"insertion_dir\"].iterdir():\n",
    "    cell_type, cell_id, tumor_type = file.stem.split(\"-\")\n",
    "    tmp_df = read_csv(file)\n",
    "    tmp_df[\"cell type\"] = cell_type\n",
    "    tmp_df[\"cell id\"] = cell_id\n",
    "    tmp_df[\"tumor type\"] = tumor_type\n",
    "    insert_list.append(tmp_df)\n",
    "inserts_df = concat(insert_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into case/controls\n",
    "insert_case = inserts_df[inserts_df[\"tumor type\"] == \"S\"]\n",
    "insert_control = inserts_df[inserts_df[\"tumor type\"] != \"S\"]\n",
    "\n",
    "# get all chromosomes to separate further the case/controls dataframes\n",
    "chrom_list = np.unique(inserts_df[\"chr\"].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph (network) of insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges(G, threshold):\n",
    "    # nodes are inherently ordered as they are added in the graph,\n",
    "    # however, the ordering doens't have to numerically make sense for this function\n",
    "    ordered_nodes = G.nodes()\n",
    "    # remove the transposon orientation from the end of the node name\n",
    "    tmp_order = [ int(x.split(\"|\")[0]) for x in ordered_nodes ]\n",
    "    # check if this changes the number of unique nodes.\n",
    "    # If we have + and - at the same location, this assert will fail.\n",
    "    # This isn't a bad thing but I want to know when it is happening\n",
    "    assert len(np.unique(ordered_nodes)) == len(np.unique(tmp_order))\n",
    "    \n",
    "    # cast the nodes into a numpy array that can be used to broadcast into a symmetric matrix of distances\n",
    "    nodes = np.array(tmp_order).reshape(-1, 1)\n",
    "    dist_nodes = np.abs(nodes - nodes.T)  # symmetric 2d array\n",
    "    \n",
    "    # cis nodes are those that are under the threshold\n",
    "    cis_nodes = dist_nodes <= threshold  # symmetric 2d array\n",
    "    \n",
    "    # get the indices of the lower left triangle of the symmetric matrix.\n",
    "    # edges_ind is a tuple of two array. The same index location in both arrays is used \n",
    "    # to index a single value from the symmetric matrix. This results in two very long \n",
    "    # arrays that will index all the values of the lower left triangle of the matrix\n",
    "    edges_ind = np.tril_indices_from(cis_nodes, k=-1) # tuple of two 1d arrays\n",
    "    \n",
    "    # keep nodes that are under the threshold\n",
    "    keep_nodes = cis_nodes[edges_ind]  # 1d array\n",
    "    \n",
    "    # set up the nodes to be a numpy array for easy indexing\n",
    "    ordered_nodes = np.array(G.nodes())  # 1d array\n",
    "    \n",
    "    # get the actual node names for the lower left triangle via as the column\n",
    "    nodes1 = ordered_nodes[edges_ind[1][keep_nodes]]  # 1d array\n",
    "    # the rows\n",
    "    nodes2 = ordered_nodes[edges_ind[0][keep_nodes]]  # 1d array\n",
    "    # and edge weights (TODO: which can be modified for a differnt weighting method, maybe 1 / log10(x) instead?)\n",
    "    nodes_dist = 1 / dist_nodes[edges_ind][keep_nodes]  # 1d array\n",
    "    # combine the nodes and weights into an iterable that can be passed wholly into the graph\n",
    "    # an edge is defined as the first node, the second node, and then a dict of attributes, such as weight\n",
    "    edges_to_add = [ (x, y, {\"weight\": z}) for x, y, z in zip(nodes1, nodes2, nodes_dist) ]\n",
    "    return edges_to_add\n",
    "\n",
    "def create_graph(chrom_df: DataFrame, threshold, save_file, verbose=0) -> None:\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # prepare the insertions by grouping them together\n",
    "    # find the total count of insertions and the counts per sequencing library (IRR/IRL)\n",
    "    insert_cols = ['chr', 'pos', 'tpn promoter orient', 'seq library']\n",
    "    tmp = chrom_df.groupby(by=insert_cols, as_index=False, dropna=False)['read name'].count()\n",
    "    tmp['count'] = tmp.pop('read name')\n",
    "    count_irr = np.where(tmp['seq library'] == 'IRR', tmp['count'], 0)\n",
    "    count_irl = np.where(tmp['seq library'] == 'IRL', tmp['count'], 0)\n",
    "    tmp.insert(5, \"count_irr\", count_irr)\n",
    "    tmp.insert(6, \"count_irl\", count_irl)\n",
    "    \n",
    "    # group insertions without the sequencing library. \n",
    "    # As long as the transposon orientation, chromosome, and position are the same, \n",
    "    # then it does not matter which library the insertion came from\n",
    "    node_cols = ['chr', 'pos', 'tpn promoter orient']\n",
    "    insertion_nodes = tmp.groupby(by=node_cols, as_index=False, dropna=False).sum(numeric_only=True)\n",
    "    insertion_nodes['read names'] = chrom_df.groupby(by=node_cols, dropna=False, group_keys=False)['read name'].apply(list).reset_index(drop=True)\n",
    "    \n",
    "    # TODO: for some reason there are few insertions that occur both in IRR and IRL\n",
    "    # both_libs = insertion_nodes[ (insertion_nodes['count_irl'] != 0) & (insertion_nodes['count_irr'] != 0) ]\n",
    "    \n",
    "    # add each insertion. Since they are unique, I can add the edges after all the nodes are in\n",
    "    for i in range(len(insertion_nodes)):\n",
    "        if (i % 1000 == 0) and (i != 0) and verbose:\n",
    "            print(f\"\\t{i+1/len(insertion_nodes)} insertions\")\n",
    "        insert = insertion_nodes.iloc[i]\n",
    "        new_node = f\"{insert['pos']}|{insert['tpn promoter orient']}\"\n",
    "        # add node(i) as an insertion location into the network\n",
    "        G.add_node(\n",
    "            new_node,\n",
    "            counts=insert[\"count\"],\n",
    "            counts_irr=insert[\"count_irr\"],\n",
    "            counts_irl=insert[\"count_irl\"],\n",
    "            orient=insert[\"tpn promoter orient\"],\n",
    "            chrom=insert[\"chr\"],\n",
    "            position=insert[\"pos\"],\n",
    "        )\n",
    "        # for other_node in G.nodes:\n",
    "        #     if other_node == new_node:\n",
    "        #         continue\n",
    "        #     # find distance between nodes using their position\n",
    "        #     node_dist = abs(G.nodes[other_node][\"position\"] - G.nodes[new_node][\"position\"])\n",
    "        #     # double check don't add edge to self\n",
    "        #     if node_dist == 0:\n",
    "        #         continue\n",
    "        #     # if distance between node(i) and node(j) is less than threshold\n",
    "        #     if node_dist <= threshold:\n",
    "        #         # add edge(ij) with a weight of the distance (or inverse?) to the network\n",
    "        #         \n",
    "        #         G.add_edge(new_node, other_node, weight=1 / node_dist)\n",
    "    # the following function does what is commented out above\n",
    "    # but I am keeping all of this in for a future user to reference\n",
    "    edges_to_add = find_edges(G, threshold)\n",
    "    G.add_edges_from(edges_to_add)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def graph_properties(G):\n",
    "    print(f\"number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"number of edges: {G.number_of_edges()}\")\n",
    "    num_inserts = 0\n",
    "    for node in G.nodes:\n",
    "        num_inserts += G.nodes[node]['counts']\n",
    "    print(f\"number of insertions: {num_inserts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 5\n",
      "number of edges: 0\n",
      "number of insertions: 5\n"
     ]
    }
   ],
   "source": [
    "# test run of toy-data\n",
    "G = create_graph(insert_case[insert_case['chr'] == \"chr1\"] , 50000, out_dir_case / \"chr1.graphml\")\n",
    "graph_properties(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of find_edges()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_generator(chrom_list, insert_case, insert_control, threshold, case_dir, control_dir) -> Generator[tuple, None, None]:\n",
    "    for chrom in chrom_list:\n",
    "        insert_case_chrom = insert_case[insert_case['chr'] == chrom]    \n",
    "        insert_control_chrom = insert_control[insert_control['chr'] == chrom]\n",
    "        case_file = case_dir / f\"{chrom}.graphml\"\n",
    "        control_file = control_dir / f\"{chrom}.graphml\"\n",
    "        yield ( insert_case_chrom, insert_control_chrom, threshold, case_file, control_file )\n",
    "\n",
    "iter_gen = create_graph_generator(chrom_list, insert_case, insert_control, args['threshold'], out_dir_case, out_dir_control)\n",
    "insert_case_chrom, insert_control_chrom, threshold, case_file, control_file = next(iter_gen)\n",
    "chrom_df = insert_case_chrom"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make graph with nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# prepare the insertions by grouping them together\n",
    "# find the total count of insertions and the counts per sequencing library (IRR/IRL)\n",
    "insert_cols = ['chr', 'pos', 'tpn promoter orient', 'seq library']\n",
    "tmp = chrom_df.groupby(by=insert_cols, as_index=False, dropna=False)['read name'].count()\n",
    "tmp['count'] = tmp.pop('read name')\n",
    "count_irr = np.where(tmp['seq library'] == 'IRR', tmp['count'], 0)\n",
    "count_irl = np.where(tmp['seq library'] == 'IRL', tmp['count'], 0)\n",
    "tmp.insert(5, \"count_irr\", count_irr)\n",
    "tmp.insert(6, \"count_irl\", count_irl)\n",
    "\n",
    "# group insertions without the sequencing library. \n",
    "# As long as the transposon orientation, chromosome, and position are the same, \n",
    "# then it does not matter which library the insertion came from\n",
    "node_cols = ['chr', 'pos', 'tpn promoter orient']\n",
    "insertion_nodes = tmp.groupby(by=node_cols, as_index=False, dropna=False).sum(numeric_only=True)\n",
    "insertion_nodes['read names'] = chrom_df.groupby(by=node_cols, dropna=False, group_keys=False)['read name'].apply(list).reset_index(drop=True)\n",
    "\n",
    "# add each insertion. Since they are unique, I can add the edges after all the nodes are in\n",
    "for i in range(len(insertion_nodes)):\n",
    "    if (i % 1000 == 0) and (i != 0) and args[\"verbose\"]:\n",
    "        print(f\"\\t{i+1/len(insertion_nodes)} insertions\")\n",
    "    insert = insertion_nodes.iloc[i]\n",
    "    new_node = f\"{insert['pos']}|{insert['tpn promoter orient']}\"\n",
    "    # add node(i) as an insertion location into the network\n",
    "    G.add_node(\n",
    "        new_node,\n",
    "        counts=insert[\"count\"],\n",
    "        counts_irr=insert[\"count_irr\"],\n",
    "        counts_irl=insert[\"count_irl\"],\n",
    "        orient=insert[\"tpn promoter orient\"],\n",
    "        chrom=insert[\"chr\"],\n",
    "        position=insert[\"pos\"],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find all edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes are inherently ordered as they are added in the graph,\n",
    "# however, the ordering doens't have to numerically make sense for this function\n",
    "ordered_nodes = G.nodes()\n",
    "ordered_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the transposon orientation from the end of the node name\n",
    "tmp_order = [ int(x.split(\"|\")[0]) for x in ordered_nodes ]\n",
    "tmp_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if this changes the number of unique nodes.\n",
    "# If we have + and - at the same location, this assert will fail.\n",
    "# This isn't a bad thing but I want to know when it is happening\n",
    "assert len(np.unique(ordered_nodes)) == len(np.unique(tmp_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the nodes into a numpy array that can be used to broadcast into a symmetric matrix of distances\n",
    "nodes = np.array(tmp_order).reshape(-1, 1)\n",
    "# find absolute difference between all pairwise nodes\n",
    "dist_nodes = np.abs(nodes - nodes.T)  # symmetric 2d array\n",
    "dist_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cis nodes are those that are under the threshold\n",
    "cis_nodes = dist_nodes <= threshold  # symmetric 2d array\n",
    "cis_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the lower left triangle of the symmetric matrix.\n",
    "# edges_ind is a tuple of two array. The same index location in both arrays is used \n",
    "# to index a single value from the symmetric matrix. This results in two very long \n",
    "# arrays that will index all the values of the lower left triangle of the matrix\n",
    "edges_ind = np.tril_indices_from(cis_nodes, k=-1) # tuple of two 1d arrays\n",
    "edges_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep nodes that are under the threshold\n",
    "keep_nodes = cis_nodes[edges_ind]  # 1d array\n",
    "keep_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the nodes to be a numpy array for easy indexing\n",
    "ordered_nodes = np.array(G.nodes())  # 1d array\n",
    "ordered_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual node names for the lower left triangle via as the column\n",
    "nodes1 = ordered_nodes[edges_ind[1][keep_nodes]]  # 1d array\n",
    "nodes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows\n",
    "nodes2 = ordered_nodes[edges_ind[0][keep_nodes]]  # 1d array\n",
    "nodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and edge weights (TODO: which can be modified for a differnt weighting method, maybe 1 / log10(x) instead?)\n",
    "nodes_dist = 1 / dist_nodes[edges_ind][keep_nodes]  # 1d array\n",
    "nodes_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the nodes and weights into an iterable that can be passed wholly into the graph\n",
    "# an edge is defined as the first node, the second node, and then a dict of attributes, such as weight\n",
    "edges_to_add = [ (x, y, {\"weight\": z}) for x, y, z in zip(nodes1, nodes2, nodes_dist) ]\n",
    "edges_to_add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netcis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
